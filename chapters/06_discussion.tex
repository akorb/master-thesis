% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Discussion}\label{chapter:discussion}

\section{Assessment of the fulfillment of requirements}

\subsection{Security requirements}

\subsection{Attestation process requirements}

% Definitions

\ac{TCG} defines as part of their Trusted Attestation Protocol~\cite{tap} the requirements for an attestation process to provide assurance to a verifier that it is (i) accurate, (ii) interpretable, and (iii) attributable.

\paragraph{Accurate}
Accurate attestation data represents the actual state of the device.
This includes freshness, i.e., the data is not replayed and does not represent an old, outdated state of the device.
While our system ensures this not alone, accuracy is established by the higher level protocol, e.g., retrieving a quote, as long as a nonce is involved.

\paragraph{Interpretable}
Intuitively, the data must be interpretable by the verifier.
In other words, the verifier must be able to derive a decision about the trustworthiness of the prover based on the attestation data.
Our system ensures that by propagating the TCIs as part of the publicly specified TCB Info Evidence extension.
Also, the TCIs constitute of a hash, and the identifier of the used hash algorithm.
Both are well-known concepts easily understandable for a verifier.

\paragraph{Attributable}
It must be possible to assign the attestation data to a specific device, i.e., it must be verifiable that the attestation data originates from the prover.
This is solved by the higher level protocol as well, since such protocols usually involve signing of attestation data, usually a quote.
The quote must be signed with the private EK corresponding to the public key in the EKcert, i.e., the leaf of the certificate chain.
And this private EK can only be created by a device that has the TCIs represented by the previously obtained certificate chain and the according UDS, which is secret and unique for the respective prover.


% \section{Compatibility of higher level protocols}
% Part of related work

\section{Implications of openly propagating system state}

% See: https://www.rfc-editor.org/rfc/rfc9334.html#section-11
An attacker could use the TCIs to find the exact version of the running software and match this to known vulnerabilities~\cite{rfc9334}.

To counteract this, the certificate chain can be transferred from the prover to the verifier via TCP/TLS, which guarantees the confidentiality of the TCI part of the certificates from eavesdroppers.
However, a malicious verifier could initiate the entire attestation process, which is then the TLS endpoint that receives the certificates.
This can be circumvented by authenticating the verifier.
However, this means that the prover only shares its state with certain verifiers.
This is a trade-off that must be weighed up by the implementers of our solution.


\section{Build pool of trusted TCIs}

In our proposed solution, the verifiers must know the TCIs they trust.
However, these reference values must be obtained from somewhere.
This is a general problem of TPM~(PCRs) and DICE~(TCIs) attestation and not specific to our system.
% If the prover's vendor uses off-the-shelf binaries 
In an ideal scenario, the software developers of boot components publish the TCIs and the corresponding security attributes.
The developers would also need to keep these listings up to date, e.g., if a vulnerability was discovered in a particular version to declare it insecure.
The verifiers can then collect these TCIs, possibly filter them again with their own security policies, and save them as their trusted TCI pool.
To date, however, we are not aware of this being common practice.
% bundle their  the TCIs of their software which are considered as secure.
% But then, the manufacturers also have to provide the matching binary of the software, except if reproducible builds are possible.
% The resulting pool might not be that huge, since there is only a tight spectrum of software fulfilling the purposes of boot firmware.

Alternatively, verifiers can also create their own trusted TCI pool.
This could be a possible solution if a verifier expects a manageable number of possible firmware on the prover.
This is the case, for example, if the verifier provisions the devices that will later be the provers themselves.

\subsection{Closed source vs.\ Open source}

It is counter-intuitive for the acquisition of TCIs that it is irrelevant whether the software is closed or open source.
In fact, it can be more difficult with open source software.
This is because the calculation of TCIs is based on the binary code and not the source code, since the binary code is eventually loaded in the memory of a device and executed.
While closed-source software must always be provided as a binary file, open-source software may only be distributed in the form of its source code.
The binary file must then be compiled by the device provisioner itself.
If its build environment does not support reproducible builds~\cite{Lamb2022}, there may be many binaries and conversely many TCIs that differ only in irrelevant details, e.g., embedded timestamps.
This is unlikely to appear with closed source software, as there is only a single distributor.

However, closed source software restricts the criteria that can be used to decide whether a TCI is trustworthy or not.
The trustworthiness of open source software can be derived independently of its source code, and also its exact change history if an older version is evaluated.
With closed source software, you have to rely on the developer of the closed-source software to communicate openly if security problems occur.


% We expect the verifier to rely on external sources to collect its pool of trustworthy TCIs.
% The most prominent example is the observation of CVEs to see if specific versions of a software contain relevant security issues.
% For open source software, the verifier has the possibility to derive a security policy based on the source code.

% Also, generally speaking, open source software receives more security audits, revealing security holes faster.
% With closed source firmware, the verifier is restricted to reverse-engineering (not necessarily doing that itself, e.g., it could also be done by security researchers), or relying on the developer to reliably publicize CVE's about its product.

% Usually, this is easier for open source software, which is not a requirement, however.

% The reference values have to come from somebody. Huge burden to the verifier, see paper of Simon for potential improvement


% \section{Hardware knowledge dependency}
% Or hardware-agnostic?

% Comparison to RATS architecture
% For RATS: Our system would be integrated by the verifier to establish trust into the fTPM, the relying party wouldn't need to know about it
% For us: verifier = relying party: full burden, needs to know everything
% but required for a (mostly) independent attestation.
% Otherwise trust relationship between verifier and relying party required
% See also Introduction of https://dl.acm.org/doi/10.1145/3600160.3600171

% See abstract of https://www.ietf.org/archive/id/draft-birkholz-rats-corim-01.html

% \section{Hardware requirements: DICE + fTPM vs.\ dTPM}
\section{Hardware requirements}

% TODO: Rework. Especially eMMC part, and abbreviations
The major advantage of firmware TPMs over dTPMs besides the better performance is that they involve less additional hardware for the final system.
They require a TEE, storage hardware supporting a Replay Protected Memory Block~(RPMB) partition, a secure world hardware fuse, and a secure entropy source~\cite{Raj2015}.
A hardware fuse, a secure entropy source, and a TEE are commonly part of the processor.
And storage has to present anyway.
Thus, it does not involve extra hardware, but a constraint for the storage type.
Support for a RPMB partition is for example provided by eMMC~\cite{eMMC} and UFS~\cite{UFS}.


% Not a secure real-time clock~(SRTC) containing the absolute physical time, but relatively to determine how much time passed.
However, our solution with DICE involves new hardware requirements.
The question at hand is whether the hardware requirements of DICE undermine the lower hardware requirements of an fTPM\@.
The purpose of DICE's hardware requirements is to protect the UDS\@.
This is usually implemented in latches as part of the processor which block access to a specific memory area after a bit has been set~\cite{Lorych2022}.
E.g., STM32G0 based on the Arm Cortex-M0+ or STM32L4 based on the Arm Cortex-M4~\cite{Lorych2022}.
For a system to be DICE-compatible, it is therefore usually sufficient for the processor to support it without the need for additional external hardware.

So, we are not cancelling out the advantage of less additional hardware components over a dedicated TPM, and instead add a restriction in the choice of processors and storage type.

% \section{Personal opinion about developed system}

% Maybe too complex?
% Bad feeling about this?
% Benefit bigger than effort?

% https://dl.acm.org/doi/pdf/10.1145/2897937.2898083
% Problems of the prover not being authenticated. This needs to be solved on top of our solution.

\section{Caveats of a static RTM}

The huge disadvantage of a static RTM is that it generally cannot provide guarantees for a system's state at the current time, but can only present the state the system inhibited at boot time~\cite{EURECOM+3536}.
However, attacks can appear during the system's runtime, which is then undetected.
Since our solution integrates DICE which does not provide capabilities of a dynamic RTM, we are restricted to a static RTM\@.
