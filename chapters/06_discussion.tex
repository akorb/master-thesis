% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Discussion}\label{chapter:discussion}

In this chapter, we discuss our findings, limitations, and recommendations.
Furthermore, we answer previously stated requirements and questions.

\section{Evaluation}

Here, we revisit and assess the research questions, security goals, and requirements for an attestation process.

\subsection{Research questions}

% In the \nameref{chapter:introduction} we listed our research questions.

\paragraph{\ref{rq:1-tpm-identity}: Identity of \ac{fTPM} (\autoref{sec:ftpm-identity})}\label{par:identity}
The first question asked is what constitutes the identity of an fTPM\@.
We found that it is essential for its identity to include its entire underlying software stack because each predecessor directly affects the security of an \ac{fTPM}.
Despite that, it is still not unique.
Therefore, it must also depend on a unique value, which we integrate by involving the \ac{UDS}.
Only by that, its identity can represent uniquely an \ac{fTPM}.
The \ac{DICE}'s component-specific \ac{CDI} represents this definition of identity.

\paragraph{\ref{rq:2-combine-infrastructure}: \acs{DICE} \texttt{+} \acs{TPM} (\autoref{sec:combine-tpm-dice})}
Next, we provide a design to combine the infrastructures of \ac{DICE} and \ac{TPM}.
Here, the crucial point is where both infrastructures meet.
While \ac{DICE} measures and derives thereon dependent secrets from the start of the device, the \ac{fTPM} is launched later in the boot chain.
When it is eventually executed, it retrieves its \ac{CDI} from \ac{DICE}, which represents the fTPM's identity as understood previously from the perspective of \ac{DICE}.
Based on that, the \ac{fTPM} derives its \ac{EPS}, which is the root seed for its \ac{EK}, which represents its identity from the perspective of \acp{TPM}.
In short, the infrastructures meet and are combined at the step from the \ac{CDI} (DICE) to the \ac{EPS} (TPM).

\paragraph{\ref{rq:3-secure-data}: Manage the fTPM's data securely (\autoref{subsec:storage-key})}
Another important aspect to take care of is managing the fTPM's data securely.
The underlying question is what \emph{securely} means in this context.
On the one hand, you want to make the data from the fTPM as widely available as possible; on the other hand, the data should be linked as closely as possible to the fTPM\@.
We choose to bind the data to an fTPM's identity, which only changes when the security properties of the fTPM change.
%, which is constant as long as the underlying firmware does not change, directly affecting the fTPM's security.
As described in~\hyperref[par:identity]{\ref*{rq:1-tpm-identity}}, the fTPM's \ac{CDI} represents this identity.
Hence, we derive a storage key from it to protect the confidentiality and integrity of the fTPM's data.

\paragraph{\ref{rq:4-privacy}: Privacy (\autoref{sec:privacy})}
Remote attestation without ensuring the privacy of the prover has the benefit of being straightforward and requiring only the expected parties---verifier and prover.
Despite that, privacy can still be a higher prioritized criterion, such that we also explain an approach to integrate it into our solution.
We introduce a proxy between the verifier and the prover---the privacy CA\@.
The prover sends its certificate chain to the privacy CA, which then verifies that the \ac{fTPM} is authentic by confirming that it roots in an authentic DICE implementation.
It then bundles the chain's information, such as measurements, into a new certificate that preserves the prover's privacy by leaving information that reveals it.
If the privacy CA honors its promise not to disclose the prover's identity and the verifier trusts the privacy CA, the prover's privacy is protected.
Ultimately, the verifier can only derive that it communicates with \emph{an} authentic TPM, but not \emph{which}, and can still decide for itself whether it considers the \ac{fTPM} to be trustworthy based on its measurements.

% The short answer to this research question is that we need a third party \ac{CA} standing between the verifier and the prover.
% The attestation process of our \ac{fTPM} involves two steps for the verifier: ensuring the \ac{fTPM} is authentic by verifying that it roots in an authentic DICE implementation (hardware-side) and that the \ac{fTPM} itself and its entire preceding firmware are trustworthy (software-side).
% The first one requires identifying the manufacturer of the prover's DICE implementation.
% We therefore move the execution of this step to a privacy \ac{CA}, while the verifier can still conduct the second step itself.

% To elaborate, the prover forwards its \ac{DICE} certificate chain containing the identity of its \ac{fTPM} to the privacy \ac{CA}, which evaluates whether it indeed represents an authentic TPM\@.
% It then lets the verifier know the DICE implementation is authentic without revealing the prover's identity.
% Hence, the verifier can only derive that it communicates with \emph{an} authentic TPM, but not \emph{which}.
% Although the verifier can still decide for itself whether it considers the \ac{fTPM} to be trustworthy based on its measurements, the disadvantage is the verifier must trust the privacy \ac{CA} to have correctly assessed the authenticity of the \ac{fTPM}.

% \paragraph{\ref{sg:5}: Privacy of remote attestation process}
% We have proposed an extension to our system to ensure the privacy of the prover during the remote attestation process.


\subsection{Security goals}

Here, we return to the security goals defined in the introduction and describe how we fulfill them.

\paragraph{\ref{sg:1}: Compromised fTPM cannot fake its identity}
The predecessor component of the fTPM, i.e., the trusted OS, performs the measurement of the fTPM, embeds it in the alias certificate representing the fTPM, and signs it with its private alias key.
If the trusted OS behaves correctly, the fTPM cannot access its private alias key, preventing it from faking its measurements or corresponding alias certificate.
The verifier establishes trust in the trusted OS's behavior by examining its \ac{TCI}\@.
In fact, it needs to trust all TCIs of the preceding components for that.
In general, each DICE layer must not have access to the private alias key of the preceding layer.

\paragraph{\ref{sg:2}: Small root of trust}
We base our system's security on the DICE architecture's design and guarantees.
Its root of trust consists of the \ac{UDS} and its protection mechanisms.
The \ac{UDS} may only be read once per operating time of the implementing device and only by DICE\@.
As a single integer value representing a statistically unique value, we consider the root of trust small.

\paragraph{\ref{sg:3}: Isolation of fTPM storage}
The problem originates from the fact that the File Encryption Key~(FEK) for the data of a TA is derived by a TEE-wide secret and the TA's UUID, with the latter being public.
Hence, any TA can derive the FEK of any other TA within the same TEE\@.
However, they cannot know the other TAs CDI since they cannot measure each other.
Consequently, this prevents other \acp{TA} from generating our solution's fTPM's storage key.
% The trusted OS calculates the CDI of the fTPM to pass it to the fTPM when the execution is handed over, but the trusted OS is also measured as part of our certificate chain.
% It can therefore be remotely verified whether it is trusted, meaning that it does not abuse its privileges.
If the trusted OS or any other preceding component is modified, e.g., to leak the fTPM's data, the CDI of the fTPM changes; consequently, its storage key and its old data cannot be accessed anymore.

\paragraph{\ref{sg:4}: Protect fTPM data against downgrade attacks on the fTPM}
The binding of the TPM data to the identity of the fTPM described for the storage isolation also protects against downgrade attacks.
Whether an fTPM is updated, downgraded, or otherwise changed with malicious intent is reflected in its CDI so that the old storage key cannot be restored.

\subsection{Attestation process requirements}

% Definitions

\Ac{TCG} defines as part of their Trusted Attestation Protocol~\cite{tap} the requirements for an attestation process to assure a verifier that it is (i) accurate, (ii) interpretable, and (iii) attributable.

\paragraph{Accurate}
Accurate attestation data represents the actual identity of the device's firmware.
This includes freshness, i.e., the data is not replayed and does not represent an outdated state of the device.
While our system ensures freshness not alone, it is established by the subsequent protocol, e.g., retrieving a quote, as long as a verifier-supplied nonce is involved.

\paragraph{Interpretable}
Intuitively, the data must be interpretable by the verifier.
In other words, the verifier must be able to decide on the prover's trustworthiness based on the attestation data.
Our system ensures that by propagating the TCIs as part of the publicly specified TCB Info Evidence extension.
Also, the TCIs consist of a hash and the corresponding hash algorithm identifier.
Both are well-known concepts that are easily understandable for a verifier.

\paragraph{Attributable}
It must be possible to assign the attestation data to a specific device, i.e., it must be verifiable that the attestation data originates from the prover.
Just like accuracy, this is also solved by the protocol that follows the attestation of the fTPM since such protocols usually involve the signing of attestation data, usually a quote.
A \ac{TPM} signs the quote with EKpriv corresponding to the EKpub in the EKcert.
And EKpriv can only be created by a device with the \acp{TCI} represented by the previously obtained certificate chain and the according \ac{UDS}, which is secret and unique for the respective prover.


% \section{Compatibility of higher level protocols}
% Part of related work

\section{Implications of openly propagating system state}

% See: https://www.rfc-editor.org/rfc/rfc9334.html#section-11
An attacker could use the TCIs to find the exact running software versions and match this to known vulnerabilities~\cite{rfc9334}.

To counteract this, the certificate chain can be transferred from the prover to the verifier via TCP/TLS, which guarantees the confidentiality of the TCI part of the certificates from eavesdroppers.
However, a malicious verifier could initiate the entire attestation process, which is then the TLS endpoint receiving the certificates.
This can be circumvented by authenticating the verifier.
However, this implies the prover only shares its attestation data with certain verifiers.
This is a trade-off that the implementers of our solution must weigh up.


\section{Build pool of trusted TCIs}

In our proposed solution, the verifiers must know the TCIs they trust.
However, these reference values must be obtained from somewhere.
This is a general problem of TPM~(PCRs) and DICE~(TCIs) attestation and is not specific to our system.
% If the prover's vendor uses off-the-shelf binaries 
In an ideal scenario, the software developers of boot components publish the TCIs and the corresponding security attributes.
The developers would also need to keep these listings up to date, e.g., if a vulnerability was discovered in a particular version to declare it insecure.
The verifiers can then collect these TCIs, possibly filter them again with their own security policies, and save them as their trusted TCI pool.
To date, however, we are not aware of this being common practice.
% bundle their  the TCIs of their software which are considered as secure.
% But then, the manufacturers also have to provide the matching binary of the software, except if reproducible builds are possible.
% The resulting pool might not be that huge, since there is only a tight spectrum of software fulfilling the purposes of boot firmware.

Alternatively, verifiers can also create their own trusted TCI pool.
This could be a possible solution if a verifier expects a manageable number of possible firmware on the prover.
This is the case, for example, if the verifier provisions the devices that will later be the provers themselves.

\subsection{Closed-source vs.\ Open-source}

It is counter-intuitive for the acquisition of TCIs that it is irrelevant whether the software is closed- or open-source.
In fact, it can be more difficult with open-source software.
This is because the calculation of TCIs is based on the binary code and not the source code since the binary code is eventually loaded in the memory of a device and executed.
While closed-source software must always be provided as a binary file, open-source software may only be distributed as its source code.
The binary file must then be generated by the device provisioner itself.
If its build environment does not support reproducible builds~\cite{Lamb2022}, there may be many binaries and, conversely, many TCIs that differ only in irrelevant details, e.g., embedded timestamps.
This problem is unlikely to appear with closed-source software, as only a single distributor exists.

However, closed-source software restricts the criteria that can be used to decide whether a TCI is trustworthy.
The trustworthiness of open-source software can be derived independently of its source code and exact change history if an older version is evaluated.
With closed-source software, you must rely on its developers to communicate openly if security problems occur.


% We expect the verifier to rely on external sources to collect its pool of trustworthy TCIs.
% The most prominent example is the observation of CVEs to see if specific versions of a software contain relevant security issues.
% For open-source software, the verifier has the possibility to derive a security policy based on the source code.

% Also, generally speaking, open-source software receives more security audits, revealing security holes faster.
% With closed-source firmware, the verifier is restricted to reverse-engineering (not necessarily doing that itself, e.g., it could also be done by security researchers), or relying on the developer to reliably publicize CVE's about its product.

% Usually, this is easier for open-source software, which is not a requirement, however.

% The reference values have to come from somebody. Huge burden to the verifier, see paper of Simon for potential improvement


% \section{Hardware knowledge dependency}
% Or hardware-agnostic?

% Comparison to RATS architecture
% For RATS: Our system would be integrated by the verifier to establish trust into the fTPM, the relying party wouldn't need to know about it
% For us: verifier = relying party: full burden, needs to know everything
% but required for a (mostly) independent attestation.
% Otherwise trust relationship between verifier and relying party required
% See also Introduction of https://dl.acm.org/doi/10.1145/3600160.3600171

% See abstract of https://www.ietf.org/archive/id/draft-birkholz-rats-corim-01.html

% \section{Hardware requirements: DICE + fTPM vs.\ dTPM}
\section{Hardware requirements}

Besides the better performance, a significant advantage of \acp{fTPM} over \acp{dTPM} is that they involve fewer hardware components for the final system.
The question at hand is whether the hardware requirements of an fTPM, with the addition of the hardware requirements of our solution, undermine this advantage.

At the introduction of \acp{fTPM} by Raj et al.\ from Microsoft~\cite{Raj2015}, they require a TEE, storage hardware supporting a Replay Protected Memory Block~(RPMB) partition, a secure world hardware fuse, and a secure entropy source for the integration of a secure \ac{fTPM}\@.
Thereof, a hardware fuse, a secure entropy source, and a TEE are commonly part of the processor without additional hardware.
An RPMB partition is required to protect the fTPM's storage from access outside the TEE and prevent rollback attacks.
Our introduction of the storage key does not prevent rollback attacks on the fTPM's data or the fTPM itself.
So, we also recommend using a trusted OS that implements secure storage on an RPMB partition, which involves the additional hardware requirement of an RPMB, which is unnecessary for \acp{dTPM}.
However, storage has to be present anyway, and commonly used storage technologies in embedded devices support RPMB partitions, e.g., eMMC~\cite{eMMC} and UFS~\cite{UFS}.

% With DICE hardware RoT we establish a binding of the fTPM's data to the fTPM's identity, without needing to trust all remaining components in the TEE\@.
% This is also not accomplished with RPMB\@.
% Instead, it protects against downgrade attacks of the fTPM's data and the fTPM itself from outside the TEE\@.

% Thus, it does not involve extra hardware, but a constraint for the storage type.
% Support for a RPMB partition is for example provided by eMMC~\cite{eMMC} and UFS~\cite{UFS}.

% Not a secure real-time clock~(SRTC) containing the absolute physical time, but relatively to determine how much time passed.

Our solution also involves the hardware requirements of DICE\@.
The purpose of its hardware requirements is to protect the \ac{UDS}\@.
According to Lorych and Jäger~\cite{Lorych2022}, this is usually implemented in latches that block access to a specific memory area after a bit has been set; such latches are commonly part of processors.
They name the STM32G0 based on the Arm Cortex-M0+ or the STM32L4 based on the Arm Cortex-M4 as examples.
For a system to be DICE-compatible, it is therefore sufficient for the processor to support it without needing additional external hardware.

So, we are not canceling out the advantage of fewer additional hardware components over a \ac{dTPM}; instead, we restrict the choice of processors and storage type.

% \section{Personal opinion about developed system}

% Maybe too complex?
% Bad feeling about this?
% Benefit bigger than effort?

% https://dl.acm.org/doi/pdf/10.1145/2897937.2898083
% Problems of the prover not being authenticated. This needs to be solved on top of our solution.

\section{Proving the fTPM runs in the TEE}

The verifier must know that the fTPM runs within the TEE, not the REE\@.
Otherwise, the fTPM would not be isolated from the components within the REE, which usually offer a large attack surface because they communicate with the Internet, for example.
The remote attestation process we propose enables this by passing the processor's name of the prover within the DeviceID certificate to the verifier.
The verifier can then understand, e.g., from the hardware's manual, that the hardware starts in the TEE and can derive from the TCIs part of the certificate chain when the system switches to the REE during the boot process.
It can therefore understand that the fTPM was launched and is running in the TEE\@.

For our proposed architecture with privacy in mind, this check has to be conducted by the privacy CA\@.
The verifier must not know the prover's hardware details since this might reveal its identity.

\section{Caveats of attesting the fTPM's identity}

The identity of a software component does not contain its current state.
While the identity of software can be derived from its binary file, assuming it is statically linked, its state is derived from a snapshot of its memory during runtime.

We attest to the identity of the fTPM and not its state.
This means we do not detect attacks that occur after the startup process, i.e., during its runtime.
