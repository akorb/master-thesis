% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Discussion}\label{chapter:discussion}

\section{Assessment of the fulfillment of requirements}

\subsection{Security goals}

Here, we return to the security goals defined in the introduction and describe how we fulfil them.

\begin{itemize}
    \item{\textbf{Compromised fTPM cannot fake its identity}\\
    The predecessor component of the fTPM, i.e., the trusted OS, performs the measurement of the fTPM, embeds it in the alias certificate representing the fTPM and signs it with its private alias key.
    As long as the trusted OS behaves correctly, the fTPM has no access to the trusted OS's private alias key.
    Therefore, the alias certificate representing the fTPM can be trusted if the TCI of the trusted OS is trustworthy since it presents the trusted OS's behavior, as then the fTPM cannot fake the certificate containing its identity.
    Actually, all TCIs of the preceding components must be trusted for that.
    In general, each component of the fTPM firmware stack does not have access to the private alias key of the component preceding it.}

    \item{\textbf{Small root of trust}\\
    We base the trust in our system on the design and guarantees of the DICE architecture.
    The root of trust of the DICE architecture consists of the UDS and its protection mechanisms.
    The UDS may only be read once per operating time of the implementing device and only by DICE\@.
    As the actual secret is a single integer value that represents a statistically unique value, we consider the root of trust to be small.}

    \item{\textbf{Isolation of fTPM storage}\\
    The problem originates from the fact that the file encryption key~(FEK) for the data of a TA is derived by a TEE-wide secret and the TA's UUID, with the latter being public.
    Hence, any TA can potentially derive the FEK of any other TA\@.
    However, they cannot know the other TA's CDI, since TA's cannot measure each other.
    Consequently, this prevents other TA's from generating the fTPM's storage key (not to be confused with the FEK).
    However, the trusted OS calculates the CDI of the fTPM to pass it to the fTPM when the execution is handed over, but the trusted OS is also measured as part of our certificate chain.
    It can therefore be remotely verified whether it is trusted, meaning that it does not abuse its privileges.
    If the trusted OS or any other preceding component is modified to leak the fTPM's data, the CDI of the fTPM changes, consequently its storage key and its old data cannot be accessed anymore.}

    \item{\textbf{Protect fTPM data from downgrade attack of the fTPM}\\
    The binding of the TPM data to the identity of the fTPM described above also protects against downgrade attacks.
    Whether an fTPM is updated, downgraded or otherwise changed with malicious intent is reflected in its CDI, so that the old storage key cannot be restored.
    }

    \item{\textbf{Privacy of remote attestation process}\\
    We have proposed an extension to our system to ensure the privacy of the prover during the remote attestation process.
    For this purpose, we introduce a proxy, the so-called privacy CA, between the verifier and the prover.
    In this process, the prover sends its certificate chain to the privacy CA, which must verify that the prover is benign, and then packages the information in a way that preserves privacy by leaving behind information that reveals the identity of the verifier.
    For example, the privacy CA must verify that the verifier's DICE implementation is authentic (with the DeviceID certificate) and that the fTPM is actually running within the TEE\@.
    This requires information about the prover's hardware, which must not be disclosed to the verifier if data protection is a concern for the prover.
    If privacy CA honors its promise not to disclose the prover's identity and the verifier trusts the privacy CA, the prover's privacy is protected.
    }
\end{itemize}

\subsection{Attestation process requirements}

% Definitions

\ac{TCG} defines as part of their Trusted Attestation Protocol~\cite{tap} the requirements for an attestation process to provide assurance to a verifier that it is (i) accurate, (ii) interpretable, and (iii) attributable.

\paragraph{Accurate}
Accurate attestation data represents the actual state of the device.
This includes freshness, i.e., the data is not replayed and does not represent an old, outdated state of the device.
While our system ensures this not alone, accuracy is established by the higher level protocol, e.g., retrieving a quote, as long as a nonce is involved.

\paragraph{Interpretable}
Intuitively, the data must be interpretable by the verifier.
In other words, the verifier must be able to derive a decision about the trustworthiness of the prover based on the attestation data.
Our system ensures that by propagating the TCIs as part of the publicly specified TCB Info Evidence extension.
Also, the TCIs constitute of a hash, and the identifier of the used hash algorithm.
Both are well-known concepts easily understandable for a verifier.

\paragraph{Attributable}
It must be possible to assign the attestation data to a specific device, i.e., it must be verifiable that the attestation data originates from the prover.
This is solved by the higher level protocol as well, since such protocols usually involve signing of attestation data, usually a quote.
The quote must be signed with the private EK corresponding to the public key in the EKcert, i.e., the leaf of the certificate chain.
And this private EK can only be created by a device that has the TCIs represented by the previously obtained certificate chain and the according UDS, which is secret and unique for the respective prover.


% \section{Compatibility of higher level protocols}
% Part of related work

\section{Implications of openly propagating system state}

% See: https://www.rfc-editor.org/rfc/rfc9334.html#section-11
An attacker could use the TCIs to find the exact version of the running software and match this to known vulnerabilities~\cite{rfc9334}.

To counteract this, the certificate chain can be transferred from the prover to the verifier via TCP/TLS, which guarantees the confidentiality of the TCI part of the certificates from eavesdroppers.
However, a malicious verifier could initiate the entire attestation process, which is then the TLS endpoint that receives the certificates.
This can be circumvented by authenticating the verifier.
However, this means that the prover only shares its state with certain verifiers.
This is a trade-off that must be weighed up by the implementers of our solution.


\section{Build pool of trusted TCIs}

In our proposed solution, the verifiers must know the TCIs they trust.
However, these reference values must be obtained from somewhere.
This is a general problem of TPM~(PCRs) and DICE~(TCIs) attestation and not specific to our system.
% If the prover's vendor uses off-the-shelf binaries 
In an ideal scenario, the software developers of boot components publish the TCIs and the corresponding security attributes.
The developers would also need to keep these listings up to date, e.g., if a vulnerability was discovered in a particular version to declare it insecure.
The verifiers can then collect these TCIs, possibly filter them again with their own security policies, and save them as their trusted TCI pool.
To date, however, we are not aware of this being common practice.
% bundle their  the TCIs of their software which are considered as secure.
% But then, the manufacturers also have to provide the matching binary of the software, except if reproducible builds are possible.
% The resulting pool might not be that huge, since there is only a tight spectrum of software fulfilling the purposes of boot firmware.

Alternatively, verifiers can also create their own trusted TCI pool.
This could be a possible solution if a verifier expects a manageable number of possible firmware on the prover.
This is the case, for example, if the verifier provisions the devices that will later be the provers themselves.

\subsection{Closed source vs.\ Open source}

It is counter-intuitive for the acquisition of TCIs that it is irrelevant whether the software is closed or open source.
In fact, it can be more difficult with open source software.
This is because the calculation of TCIs is based on the binary code and not the source code, since the binary code is eventually loaded in the memory of a device and executed.
While closed-source software must always be provided as a binary file, open-source software may only be distributed in the form of its source code.
The binary file must then be compiled by the device provisioner itself.
If its build environment does not support reproducible builds~\cite{Lamb2022}, there may be many binaries and conversely many TCIs that differ only in irrelevant details, e.g., embedded timestamps.
This is unlikely to appear with closed source software, as there is only a single distributor.

However, closed source software restricts the criteria that can be used to decide whether a TCI is trustworthy or not.
The trustworthiness of open source software can be derived independently of its source code, and also its exact change history if an older version is evaluated.
With closed source software, you have to rely on the developer of the closed-source software to communicate openly if security problems occur.


% We expect the verifier to rely on external sources to collect its pool of trustworthy TCIs.
% The most prominent example is the observation of CVEs to see if specific versions of a software contain relevant security issues.
% For open source software, the verifier has the possibility to derive a security policy based on the source code.

% Also, generally speaking, open source software receives more security audits, revealing security holes faster.
% With closed source firmware, the verifier is restricted to reverse-engineering (not necessarily doing that itself, e.g., it could also be done by security researchers), or relying on the developer to reliably publicize CVE's about its product.

% Usually, this is easier for open source software, which is not a requirement, however.

% The reference values have to come from somebody. Huge burden to the verifier, see paper of Simon for potential improvement


% \section{Hardware knowledge dependency}
% Or hardware-agnostic?

% Comparison to RATS architecture
% For RATS: Our system would be integrated by the verifier to establish trust into the fTPM, the relying party wouldn't need to know about it
% For us: verifier = relying party: full burden, needs to know everything
% but required for a (mostly) independent attestation.
% Otherwise trust relationship between verifier and relying party required
% See also Introduction of https://dl.acm.org/doi/10.1145/3600160.3600171

% See abstract of https://www.ietf.org/archive/id/draft-birkholz-rats-corim-01.html

% \section{Hardware requirements: DICE + fTPM vs.\ dTPM}
\section{Hardware requirements}

A major advantage of firmware TPMs over dedicated TPMs besides the better performance is that they involve less hardware components for the final system.
The question at hand is whether the hardware requirements of an fTPM with the addition of the hardware requirements of our solution undermine this advantage.

At the introduction of firmware TPMs by Raj et al.\ from Microsoft~\cite{Raj2015}, they require a TEE, storage hardware supporting a Replay Protected Memory Block~(RPMB) partition, a secure world hardware fuse, and a secure entropy source for the integration of a secure firmware TPM\@.
Thereof, a hardware fuse, a secure entropy source, and a TEE are commonly part of the processor without the need for additional hardware.
Raj et al.\ require a RPMB partition to protect the fTPM's storage from access outside the TEE, and prevent rollback attacks.
Rollback attacks of the fTPM's data or the fTPM itself are not prevented by our introduction of the storage key.
So, we also recommend using a trusted OS implementing secure storage on a RPMB partition, which involves the additional hardware requirement of an RPMB which is not necessary for dedicated TPMs.
However, storage has to present anyway, and a RPMB partition is supported  by storage technologies commonly used in embedded devices, e.g., eMMC~\cite{eMMC} and UFS~\cite{UFS}.

% With DICE hardware RoT we establish a binding of the fTPM's data to the fTPM's identity, without needing to trust all remaining components in the TEE\@.
% This is also not accomplished with RPMB\@.
% Instead, it protects against downgrade attacks of the fTPM's data and the fTPM itself from outside the TEE\@.

% Thus, it does not involve extra hardware, but a constraint for the storage type.
% Support for a RPMB partition is for example provided by eMMC~\cite{eMMC} and UFS~\cite{UFS}.

% Not a secure real-time clock~(SRTC) containing the absolute physical time, but relatively to determine how much time passed.

Our solution also involves the hardware requirements of DICE\@.
The purpose of its hardware requirements is to protect the UDS\@.
According to Lorych and JÃ¤ger~\cite{Lorych2022}, this is usually implemented in latches which block access to a specific memory area after a bit has been set; such latches are commonly part of processors.
They name the STM32G0 based on the Arm Cortex-M0+ or the STM32L4 based on the Arm Cortex-M4 as examples.
For a system to be DICE-compatible, it is therefore usually sufficient for the processor to support it without the need for additional external hardware.

So, we are not cancelling out the advantage of less additional hardware components over a dedicated TPM, and instead we add restriction in the choice of processors and storage type.

% \section{Personal opinion about developed system}

% Maybe too complex?
% Bad feeling about this?
% Benefit bigger than effort?

% https://dl.acm.org/doi/pdf/10.1145/2897937.2898083
% Problems of the prover not being authenticated. This needs to be solved on top of our solution.

\section{Proving the fTPM runs in the TEE}

It is important for the verifier to know that the fTPM runs within the TEE and not the REE\@.
Otherwise, the fTPM would not be isolated from the components within the REE, which usually offer a large attack surface because they communicate with the Internet, for example.
The remote attestation process we propose enables this by passing the processor's name of the prover within the DeviceID certificate to the verifier.
The verifier can then understand, e.g., from the hardware's manual, that the hardware starts in the TEE and can derive from the TCIs part of the certificate chain when the system switches to the REE\@.
It can therefore understand that the fTPM is running in the TEE\@.

For our proposed architecture with privacy in mind, this check has to be conducted by the privacy CA\@.
That is due to that the verifier must not know details of the prover's hardware, since this might reveal its identity.

\section{Caveats of a static RTM}

The huge disadvantage of a static RTM is that it generally cannot provide guarantees for a system's state at the current time, but can only present the state the system inhibited at boot time~\cite{EURECOM+3536}.
However, attacks can appear during the system's runtime, which is then undetected.
Since our solution integrates DICE which does not provide capabilities of a dynamic RTM, we are restricted to a static RTM\@.
